# Resnet

자랑은 짧게

- 2015년 ILSVRC, COCO 대회 분류 및 탐지 모든 부문에서 우승
- 처음으로 이미지 분류에 대한 인간의 평균 인지 능력을 추월

# 1. 설계 사상

## 1-1. Resnet이 주목한 CNN 모델의 문제점

- 층이 깊은 모델일 수록 성능이 더 좋아져야 하는 것 아닌가?

![Untitled](Resnet%200a24061cbb7b4301a849c2f4c49db5b6/Untitled.png)

1. 그레디언트 소실/폭발 문제 때문? - 가중치 초기화, ReLU, BN 등으로 개선 되었다!
2. 오버피팅 때문? - train, test 모두 깊을 수록 에러가 크다! - 오버피팅 아니다!
3. 완전 새로운 문제(Degration Problem) 이다!  - 새로운 접근법을 찾아보자!
    1. 모델을 깊게 하더라도 최소한 얕은 모델 이상의 성능이 나오게 할 수는 없을까?
        - 항등 매핑! - 극단적임
            
            ![Untitled](Resnet%200a24061cbb7b4301a849c2f4c49db5b6/Untitled%201.png)
            
        - 더 나아가
            
            **학습이 필요하면** 입/출력 매핑을 학습하고! - 항등 매핑 + 학습 매핑
            
            **학습이 필요 없으면** 입력을 그대로 통과하자! - 항등 매핑
            
            ![Untitled](Resnet%200a24061cbb7b4301a849c2f4c49db5b6/Untitled%202.png)
            

## 1-2. 레즈 블록의 특징

![Untitled](Resnet%200a24061cbb7b4301a849c2f4c49db5b6/Untitled%203.png)

- x를 입력으로 받아 만들고 싶은 이상적인 출력이 H(x)라 할 때
    
    단지 F(x) 로 만들기  **VS**  x(항등 매핑) + F(x)로 만들기
    
- 극단적으로 가정해서 이상적인 출력이 x라면! → 즉, H(x) = x 이어야 한다면!
    
    F(x) 로 x 만들기  **VS**  F(x)로 0 만들기
    
    - F(x)의 가중치는 일반적으로 평균이 0으로 초기화 된다 - 후자(레즈 블록) 승!
- 레이어가 깊을수록 **차근차근 조금씩** 값을 업데이트 하는게 이상적일 것
    - 그런 의미에서 x(항등 매핑)을 default로 주고
        
        추가로 잔차(H(x)-x)를 학습하는게 효율적
        
    - 실제로 층이 깊을수록 차근차근 조금씩 학습할까?
        - 다음은 각 층에서 출력의 std(Standard deviations)이다.
        - ResNet일 경우 깊은 층을 가질수록 std가 전체적으로 더 작음
        - 즉, 깊을수록 더 조금씩 학습해 나간다.
    
    ![Untitled](Resnet%200a24061cbb7b4301a849c2f4c49db5b6/Untitled%204.png)
    
- F(x)가 H(x) 를 학습하는 문제를 —> 잔차( H(x)-x ) 를 학습하는 문제로 변형!
    - F(x)가 풀어야 할 문제가 더 쉽게 정의됨 - 진짜로?
        
        ![Untitled](Resnet%200a24061cbb7b4301a849c2f4c49db5b6/Untitled%205.png)
        
    
    ![Untitled](Resnet%200a24061cbb7b4301a849c2f4c49db5b6/Untitled%206.png)
    
    - 위 그림들은 추후(2018년)에 발표된 내용
    - NS(no skip-connetion) 이면 층이 깊어질수록 Loss 함수가 더 복잡해짐
    - But yes skip-connetion 이면 비교적 예쁘게 유지됨

# 2. 설계 구조

## 레즈 블럭

왼쪽(Basic Block): 50층 미만일 경우,                            오른쪽(BottleNeck Block): 50층 이상일 경우

![Untitled](Resnet%200a24061cbb7b4301a849c2f4c49db5b6/Untitled%207.png)

![Untitled](Resnet%200a24061cbb7b4301a849c2f4c49db5b6/Untitled%208.png)

![Untitled](Resnet%200a24061cbb7b4301a849c2f4c49db5b6/Untitled%209.png)

- **Basic Block** : 50층 미만의 얕은 층의 경우
    - 출력 순서
        1. input 채널 수와 Conv 커널 수가 **k**로 같을 경우
            - input: X(**n by n**, **k**개 채널)
            - —> Conv( 3*3, stride=1, padding=1, 커널수=**k**개 ) —> BN —> Relu
            - —> Conv( 3*3, stride=1, padding=1, 커널수=**k**개 ) —> BN —> +X —> Relu
            - output: **n by n**, **k**개 채널
            - 이때 +X는 identity shortcut(항등 매핑)
        2. input 채널 수와 Conv 커널 수가 서로 다를 경우 - 채널 수와 resolution이 바뀌는 층
            - input: X(**n by n**, **k**개 채널)
            - —> Conv( 3*3, **stride=2**, padding=1, 커널수=**2k**개 ) —> BN —> Relu
            - —> Conv( 3*3, stride=1, padding=1, 커널수=**2k**개 ) —> BN
            - —> +X**[ Conv( 1*1, stride=2, padding=1, 커널수=2k개 ) —> BN ]** —> Relu
            - output: **n/2 by n/2**, **2k**개 채널
            - 이때 +X는 projection shortcut(채널 수와 resoltion 조절)
            - Conv3_1, Conv4_1, Conv5_1 과 같이 채널 수가 줄어드는 첫 블럭일 때 사용된다.
    - input 채널 수와 Conv 커널 수가 달라지는 블럭이 있는 이유
        - Resnet에서는 풀링을 2개로 최소한으로 사용한 대신
        - stride를 2로 주어 서브샘플링 하게 된다.
        - 즉, 서브샘플링과 Conv 연산을 동시에 하는 블럭임
        - Resolution은 반쪽이 되고, 채널 수는 2배가 된다.
        - Conv3_1, Conv4_1, Conv5_1 과 같이 채널 수가 줄어드는 첫 블럭일 때 사용된다.
        - +X도 1*1 Conv연산으로 채널 수와 resolution을 조절해 주어야 함
    - 3*3 Conv을 2개로 한 이유
        - 수용 영역(receptive field)은 유지하되 파라미터 수는 줄일 수 있음.
        - 5*5 = (3*3) * (3*3) —> 수용 영역은 5로 같음, 파라미터는 25>18
        - VGGNet의 설계 사상임
- **BottleNeck Block** : 50층 이상의 깊은 층의 경우
    - 출력 순서
        1. input 채널 수 **4k**와 **마지막** Conv 커널 수 **4k**가 같을 경우
            - input: X(**n by n**, **4k**개 채널)
            - —> Conv( 1*1, stride=1, padding=1, 커널수=**k**개 ) —> BN —> Relu
            - —> Conv( 3*3, stride=1, padding=1, 커널수=**k**개 ) —> BN —> Relu
            - —> Conv( 1*1, stride=1, padding=1, 커널수=**4k**개 ) —> +X —> Relu
            - output: **n by n**, **4k**개 채널
            - 이때 +X는 identity shortcut(항등 매핑)
        2. input 채널 수가 **2k**이고 **마지막** Conv 커널 수가 **4k**로 다를경우
            
             - 채널 수와 resolution이 바뀌는 층
            
            - input: X(**n by n,** **2k**개 채널)
            - —> Conv( 1*1, stride=1, padding=1, 커널수=**k**개 ) —> BN —> Relu
            - —> Conv( 3*3, **stride=2**, padding=1, 커널수=**k**개 ) —> BN —> Relu
            - —> Conv( 3*3, stride=1, padding=1, 커널수=**4k**개 ) —> BN
            - —> +X**[ Conv( 1*1, stride=2, padding=1, 커널수=4k개 ) —> BN ]** —> Relu
            - output: **n/2 by n/2**, **4k**개 채널
            - 이때 +X는 projection shortcut(채널 수와 resoltion 조절)
            - Conv3_1, Conv4_1, Conv5_1 과 같이 채널 수가 줄어드는 첫 블럭일 때 사용된다.
        3. Conv2_1로 첫 번째 레즈 블록일 경우 - 채널 수만 바뀌고 **resolution은 그대로**
            - input 채널 수가 **64**이고 **마지막** Conv 커널 수가 64*4일 경우임 - **stride = 1 주의**
            - input: X(**n by n,** **64**개 채널)
            - —> Conv( 1*1, stride=1, padding=1, 커널수=**64**개 ) —> BN —> Relu
            - —> Conv( 3*3, **stride=1**, padding=1, 커널수=**64**개 ) —> BN —> Relu
            - —> Conv( 3*3, stride=1, padding=1, 커널수=**64*4**개 ) —> BN
            - —> +X**[ Conv( 1*1, stride=1, padding=1, 커널수=64*4개 ) —> BN ]** —> Relu
            - output: **n by n**, **64*4**개 채널
            - 이때 +X는 projection shortcut(채널 수와 resoltion 조절)
    - 2번과 3번의 차이점
        - 2번은 Conv3_1, Conv4_1, Conv5_1 에서 나타나며,
            
            resolution이 반쪽이 되고, 채널 수는 2배가 된다.
            
            그래서 stride =2 이다.
            
        - 3번은 Conv2_1 에서만 나타나며,
        resolution은 그대로 이고, 채널 수는 4배가 된다.
            
            그래서 stride=1 이다.
            
    - 1*1 Conv을 사용한 이유
        - 층이 깊어지면서 많아지는 파라미터를 줄여 효율적으로 연산을 하기 위함
            - 첫 1*1 Conv으로 채널 수를 줄이고,
            - 3*3 Conv으로 연산한 후,
            - 다시 1*1 Conv으로 채널 수를 늘려줌
        - 1*1 Conv으로 채널 특징만 추출 가능
        - 1*1 Conv은 점별 콘벌루션(Pointwise Convolution)이라고 부름

# 3. 구현

[https://drive.google.com/file/d/1htXploXW2RT3jV1Wy8Z5t2qRcUYpR39J/view?usp=sharing](https://drive.google.com/file/d/1htXploXW2RT3jV1Wy8Z5t2qRcUYpR39J/view?usp=sharing)